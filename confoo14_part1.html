<html>
<head>
  <link href='http://fonts.googleapis.com/css?family=Inconsolata|Merriweather+Sans:700italic,300|Source+Sans+Pro:300' rel='stylesheet' type='text/css'/>
  <link href="style.css" rel="stylesheet" type="text/css"/>
  <meta name="viewport" content="width=640"/>

  <title>aGHz - Wherein I attend Confoo</title>
</head>
<body>
<div class="container container_12">

  <div class="grid_2 menu">
    <h1><a href="/" class="title">aGHz</a></h1>
    <h2>Adrian Ghizaru</h2>
    <div class="buttons">
      <a href="https://twitter.com/_aGHz/"><i class="icon icons-twitter"></i></a>
      <a href="http://ca.linkedin.com/in/adrianghizaru/"><i class="icon icons-linkedin"></i></a>
      <a href="https://github.com/aGHz"><i class="icon icons-github"></i></a>
    </div>
  </div>

  <div class="grid_block grid_10 push_2 blog_post">

<!-- POST CONTENT -->

<h1><a href="confoo14_part1" class="title">Wherein I attend Confoo</a></h1>

<p>At the end of February, Montreal played host to our very own international web technology conference. <a href="http://confoo.ca">Confoo</a> is my very first web conference, so what better way to kick-start this blog than with a recap of my experience.</p><div class="se-section-delimiter"></div>

<h2 id="before-the-conference">Before the conference</h2>

<p>The ticket for Confoo, priced at a hefty $845, was graciously provided by the <a href="http://nfb.ca">National Film Board of Canada</a>, who sends its web developers to all local conferences and some remote ones too. They’re very awesome like that. The preparation though wasn’t a pleasant experience, due mainly to the sad UX on confoo.ca. For one, you can’t select multiple topics to highlight the pertinent talks on their <a href="http://confoo.ca/en/2014/schedule">schedule page</a>, and you need to be logged in to be able to star them. Speaking of which, every time I tried to log in on my iPhone their server panicked because <em>“CSRF attack detected”</em>. I am <code>1337 hax0r, ph33r me</code>. Good thing they provided these large ad-filled dead-tree booklets to carry around.</p>

<p>Seriously though, none of that really mattered because I was only there to be dazzled with fresh knowledge. Word of caution first: <em>if you know anything about a topic, don’t attend a Confoo talk on the subject</em>. They purposefully select their presentations to stay at a general level that’s easy to grasp. So instead go learn exciting new things, like <a href="http://confoo.ca/en/2014/session/the-perl-renaissance">what Perl’s been quietly up to these last few years</a>, <a href="http://confoo.ca/en/2014/session/linked-data-publication-and-consumption">linked data</a> or <a href="http://confoo.ca/en/2014/session/integrated-cache-invalidation-for-better-hit-ratios">crazy caching voodoo using Varnish and PostgreSQL</a>.</p>

<p>Without further ado, here are my thoughts on the talks I’ve seen with a few words at the end (of part two) about the ones I would’ve liked to see.</p><div class="se-section-delimiter"></div>

<h2 id="exobrain">Exobrain</h2>

<p>I kicked off the conference with Paul Fenwick’s guided tour of his open source <a href="https://ifttt.com/">If-This-Then-That</a> clone in Perl. It lacks the usability of IFTTT but more than makes up for it by giving you the ability to write your own agents, thus circumventing the <a href="http://updates.ifttt.com/post/31945038639/upcoming-changes-to-twitter-triggers">petty</a> political <a href="http://mashable.com/2012/12/05/twitter-instagram-issues/">posturing</a> so common among Web 2.0 giants these days. And since all the components talk to each other in JSON over ZMQ, you could theoretically plug in agents written in any language. Paul demonstrated how tweeting at him with #todo adds an item to his list and pushes it to his watch. I of course (ab)used the system to volunteer <a href="https://twitter.com/_aGHz/status/438697178458898433">a very helpful recommendation</a>. He also uses it to get his Beeminder and his HabitRPG to talk to each other, so that’s nice too.</p>

<p>During the unconference period on the last day, a few of us had the honour of booting exobrain up for the <small>(allegedly)</small> very first time on machines that didn’t belong to Paul. The procedure isn’t for the faint of heart, and if you’re not already using your environment for routine Perl development, installing all the dependencies takes nigh on <a href="https://github.com/pjf/exobrain/issues/17">half an hour</a>. But when it was done, it was glorious and now I get to rekindle my Perl romance and explore its <em>renaissant</em> ecosystem. Link dump from my notes: <a href="http://habitrpg.com">HabitRPG</a>, <a href="http://search.cpan.org/~miyagawa/App-cpanminus-1.7001/bin/cpanm">cpanminus</a> for a better CPAN cli, <a href="https://github.com/berekuk/Ubic">Ubic</a> service manager, <a href="http://www.floodgap.com/software/ttytter/">ttyter</a>, <a href="http://www.vimperator.org/vimperator">vimperator</a> Firefox plugin, <a href="http://www.zotero.org/">Zotero</a> the personal research assistant.</p>

<p>Paul blogs at <a href="http://pjf.id.au/">pjf.id.au</a> and tweets as <a href="https://twitter.com/pjf">@pjf</a>, and you can find Exobrain on <a href="https://github.com/pjf/exobrain">GitHub</a> and on <a href="https://groups.google.com/forum/#!forum/exobrain">Google Groups</a>.</p><div class="se-section-delimiter"></div>

<h2 id="web-scraping-for-fun-and-profit">Web scraping (for fun <em>and</em> profit)</h2>

<p>I absolutely love web scraping, it makes me feel like some kind of data liberator. It’s an intricate labour of hack and craft, so whenever you get to hear a fellow scraper talk, it’s like peeking inside an artisan’s secret toolbox. <a href="http://benlamb.com/">Ben Lamb</a> definitely delivered on that front, sharing war stories of crawling retail websites for price watching and venue sites for event info. Interesting tools in his kit: a proxy like <a href="http://www.telerik.com/fiddler">Fiddler</a> for spying on requests by XHR or Flash, <a href="http://pyparsing.wikispaces.com/">PyParsing</a> for extracting data from natural language (in his case <a href="https://github.com/Zurgy/DateParser">dates</a>) and RabbitMQ for orchestrating spiders.</p>

<p>He also had some great advice to give. Most importantly, prepare a generic base crawler that provides you with tools you’ll want in all your spiders. Think about <em>graceful degradation</em> <small>(because human writing is anything but clean and structured)</small>, <em>logging</em> with plenty of diagnostic info <small>(because human writing…)</small>, <em>data normalization</em> <small>(because human writing…)</small>, <em>data replay</em> (saving the scraper state so it can be unpaused once the issue is fixed. <small>You know, because human writing…</small>) and <em>orchestration controls</em> so you can play with the spiders. I haven’t yet looked into <a href="http://scrapy.org/">Scrapy</a> but I believe it provides at least some of this. Then you want a good test suite for the various bits of parsing you do. Whenever you come across a new bit of data that you want from the wild wild web, you can add it to the suite and ensure your code can handle it (e.g. the various flavours that dates may come in). Finally, be nice to the sites you’re scraping and take a 5-10 second breath between requests, even if for no other reason than to avoid detection.</p>

<p>Ben is on Twitter as <a href="https://twitter.com/zurgy">@zurgy</a> and his slides are  <a href="http://jakeaustwick.me/python-web-scraping-resource/">here</a>. And if scraping is your vice, you might also be interested in this <a href="http://jakeaustwick.me/python-web-scraping-resource/">Python web scraping resource</a>.</p><div class="se-section-delimiter"></div>

<h2 id="linked-data">Linked data</h2>

<p>I’ve only tangentially looked at linked data before when investigating ways to add <a href="http://amundsen.com/blog/archives/1109">hypermedia affordances</a> to JSON, and <a href="http://json-ld.org/">JSON-LD</a> popped up. <a href="http://csarven.ca/">Sarven Capadisli</a> presented an excellent primer to linked data concepts and tools on the first day, and a case study on using linked statistical data on the second. We started with Tim Berners-Lee’s <a href="http://www.w3.org/DesignIssues/LinkedData.html">four principles</a> driving linked data. We moved on to the data model used for concretizing these principles, <a href="http://en.wikipedia.org/wiki/Resource_Description_Framework">RDF</a>, and to its basic unit of structure, the triple <code>&lt;subject, predicate, object&gt;</code>. These triples can be serialized in a host of formats, of which we saw <a href="http://en.wikipedia.org/wiki/RDFa">RDFa</a>, <a href="http://en.wikipedia.org/wiki/Turtle_%28syntax%29">Turtle</a>, <a href="http://en.wikipedia.org/wiki/N-Triples">N-Triples</a> and <a href="http://en.wikipedia.org/wiki/RDF/XML">RDF/XML</a>. To ensure RDF actually <em>links</em> things together, the <a href="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/#section-triples">triple items</a> are IRIs, and lists of IRIs for common concepts have been compiled as vocabularies (or <a href="http://en.wikipedia.org/wiki/Ontology_%28information_science%29">ontologies</a>), e.g. <a href="http://en.wikipedia.org/wiki/FOAF_%28ontology%29">FOAF</a> for social concepts, <a href="http://dublincore.org/">Dublin Core</a> for web resources and similar real-world artifacts, and <a href="http://schema.org/docs/documents.html">schema.org</a>.</p>

<p>We then looked at some ways you can interact with RDF data. <a href="http://librdf.org/raptor/rapper.html">rapper</a> is a cli tool for parsing RDF (in RDF/XML, N-Triples, Turtle or RSS/ATOM notations) based on the <a href="http://librdf.org/">Redland</a> RDF libraries. <a href="http://www.w3.org/TR/sparql11-overview/">SPARQL</a> is a query language and protocol for interacting with RDF graphs on the web or in an RDF store (i.e. a db for triples).</p>

<p>Illustrating the power of linked data, Sarven built a web interface for performing <a href="http://stats.270a.info/">statistical analysis</a> on open statistical data sets from large organizations like the <a href="http://worldbank.270a.info/.html">World Bank</a> and <a href="http://transparency.270a.info/.html">Transparency International</a>. You can learn more about his work on <a href="http://csarven.ca/statistical-linked-dataspaces">statistical linked dataspaces</a> at <a href="http://270a.info/">270a.info</a>.</p><div class="se-section-delimiter"></div>

<h2 id="js-testing">JS testing</h2>

<p>Jordan <a href="https://twitter.com/jakerella">@jakerella</a> Kasper warned against pretending that eyeballing your site in a browser can substitute for disciplined testing. We’re already passing our Angular code through the Jasmine wringer at the NFB so not much here was new to us. But if you’re still writing cowboy javascript, Jordan has a few useful tips: <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/function#Named_function_expression">name</a> your callback functions so you can get useful information in tracebacks, don’t couple your code to DOM queries but pass already selected elements to your functions instead, and don’t couple your function internals to the server by dropping some XHR in the middle of it all. Here are the <a href="http://presentboldly.com/jakerella/javascript-testing">slides</a>, and notice the beautiful <a href="http://presentboldly.com/">Blazon</a> presentation tool he’s been building at appendTo.</p>

<p>Anyway, turns out Jordan’s a great guy, very fun to have beers with, and the only other human being I know of to have a Matrix skull plug like mine. Represent.</p>

<hr>

<p>That was it for the first day. Come back for part two, the adventure continues.</p>

<!-- / POST CONTENT -->

  </div>

<div class="clearfix">&nbsp;</div>

</div>
</body>
</html>
